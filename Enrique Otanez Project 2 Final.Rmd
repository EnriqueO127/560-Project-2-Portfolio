---
title: "Enrique Otanez Project 2 RMD"
author: "Enrique Otanez"
date: "3/16/2021"
output: word_document
---

````{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_knit$set(root.dir = normalizePath("D:/Templates/UW Stuff/Classes/MSBA/Classes/TBLANT 560/Project 2"))
```

```{r}
#to work with the dataset you need to use the code on line 15 to retrieve it.
require(mlbench)
data(BreastCancer)

#Here you bind the columns you want to work with.
BreastCancer
BreastCancer <- cbind(BreastCancer[8],BreastCancer[1:11]) 
BreastCancer

#By identifying the NA's and knowing that they are low in number, complete cases is used to remove entire rows of data. 
sum(is.na(BreastCancer))
BreastCancer <- BreastCancer[complete.cases(BreastCancer), ]
BreastCancer

#We can see that we do not need the duplicated column when we cbinded the columns or the ID because of having low predictive value. 
BreastCancer <- subset(BreastCancer, select = -c(1,2))
BreastCancer

#Here we run our first model a SVM and we predict our label class.
library(e1071)
mysvm <- svm(Class ~ ., BreastCancer)
mysvm.pred <- predict(mysvm, BreastCancer)
length(mysvm.pred)
length(BreastCancer$Class)
table(mysvm.pred,BreastCancer$Class)


#The output shows that we have a classifier that predicts very well. 
```


```{r}
#Here, and for the rest of the models, we do the same thing. Use a different method of classification and then predict with the model. 
#This model here is naive bayes classification. 
library(klaR)
mynb <- NaiveBayes(Class ~ ., BreastCancer)
mynb.pred <- predict(mynb, BreastCancer)
mynb.df <- data.frame(mynb.pred)
mynb.df
length(mynb.pred$Class)
length(BreastCancer$Class)
table(mynb.pred$class,BreastCancer$Class)

```


```{r}
#This model is a neural network classification model.
library(nnet)
mynnet <- nnet(Class ~ ., BreastCancer, size=1)
mynnet.pred <- predict(mynnet,BreastCancer,type="class")
table(mynnet.pred,BreastCancer$Class)
str(mynnet.pred)
```


```{r}
library(MASS)

#Here we see a decision tree model for classification, the plot provided helps show the relationships. 
#Decision trees
library(rpart)
mytree <- rpart(Class ~ ., BreastCancer)
plot(mytree); text(mytree) # in "iris_tree.ps"
summary(mytree)
mytree.pred <- predict(mytree,BreastCancer,type="class")
table(mytree.pred,BreastCancer$Class)
```



```{r}
#Here is a leave-1-out cross validation model. 
# Leave-1-Out Cross Validation (LOOCV)
ans <- numeric(length(BreastCancer[,1]))
for (i in 1:length(BreastCancer[,1])) {
  mytree <- rpart(Class ~ ., BreastCancer[-i,])
  mytree.pred <- predict(mytree,BreastCancer[i,],type="class")
  ans[i] <- mytree.pred
}
ans <- factor(ans,labels=levels(BreastCancer$Class))
table(ans,BreastCancer$Class)

#This model lis marginally better than the previous model. 
```


```{r}

#Here is a quadratic discriminant analysis.
#Quadratic Discriminant Analysis
library(MASS)
str(BreastCancer)
for (i in 1:ncol(BreastCancer)){
  BreastCancer[,i] <- as.numeric(BreastCancer[,i])
}
BreastCancer$Class <- as.factor(BreastCancer$Class)
BreastCancer <- data.frame(BreastCancer)
str(BreastCancer)
myqda <- qda(Class ~ ., BreastCancer)
myqda.pred <- predict(myqda, BreastCancer)
table(myqda.pred$class,BreastCancer$Class)

```


```{r}

# A regularised Discriminant Analysis
library(klaR)
myrda <- rda(Class ~ ., BreastCancer)
myrda.pred <- predict(myrda, BreastCancer)
table(myrda.pred$class,BreastCancer$Class)

```


```{r}
#Lastly, a random forest. 
#Random Forests
library(randomForest)
myrf <- randomForest(Class ~ .,BreastCancer)
myrf.pred <- predict(myrf, BreastCancer)
table(myrf.pred, BreastCancer$Class)
# myrf.pred    setosa versicolor virginica
#   setosa     50      0          0
#   versicolor  0     50          0
#   virginica   0      0         50
# (Suspiciously correct! - need to read the manual)

```
```{r}
#Here we are combining the predictions of each model except mypda.pred. There is some error here and the results without including the model are well enough. 
combine.classes<-data.frame(myrf.pred, myrda.pred$class,#myqda.pred,
mytree.pred,mynnet.pred,mysvm.pred, mynb.pred$class)

#We take a look at the data. 
head(combine.classes)
head(myrf.pred)
head(myrda.pred)

#We make sure that the data types will run.
str(combine.classes)

#Here we start to change each of the predictions by changing it to either 0 or 1. 
combine.classes$myrf.pred<-ifelse(combine.classes$myrf.pred=="benign", 0, 1)
combine.classes[,2]<-ifelse(combine.classes[,2]=="benign", 0, 1)
combine.classes[,3]<-ifelse(combine.classes[,3]=="benign", 0, 1)
combine.classes[,4]<-ifelse(combine.classes[,4]=="benign", 0, 1)
combine.classes[,5]<-ifelse(combine.classes[,5]=="benign", 0, 1)
combine.classes[,6]<-ifelse(combine.classes[,6]=="benign", 0, 1)

#Here, we take the majority vote rule by using the rowSums function on the combine.classes.
majority.vote=rowSums(combine.classes)
head(majority.vote)

#We do the rowSums on the last two columns in the data.frame, this is because their output was slightly different than the other models and as such needed to be 
#treated differently.
combine.classes[,7]<-rowSums(combine.classes)
combine.classes[,8]<-ifelse(combine.classes[,7]>=4, "malignant", "benign")

#Here we have the ensemble table.
table(combine.classes[,8], BreastCancer$Class)



```


